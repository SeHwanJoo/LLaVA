# ğŸš€ Video Vision-Language Model Training Repository

## ğŸ“Œ Overview  
This repository provides an implementation of a Vision-Language Model (VLM) for video-based tasks. It supports dataset preprocessing, model training.

## âœ¨ Features  
- Efficient training with distributed computing
- LLaVA based architecture

## ğŸ“‚ Directory Structure  
```
â”œâ”€â”€ ğŸ“‚ checkpoints                        # checkpoint directory for trained models
â”‚   â”œâ”€â”€ ğŸ“‚llava-v1.5-13b-pretrain
â”‚   â””â”€â”€ ğŸ“‚llava-v1.5-13b-pretrain-video
â”œâ”€â”€ ğŸ“‚ configs                            # configuration files (.yaml)
â”‚   â”œâ”€â”€ ğŸ“‚ data_args                      # config for data
â”‚   â”œâ”€â”€ ğŸ“‚ experiments                    # config for experiments config
â”‚   â”œâ”€â”€ ğŸ“‚ model_args                     # config for models
â”‚   â””â”€â”€ ğŸ“‚ training_args                  # config for train
â”œâ”€â”€ ğŸ“‚ llava
â”‚   â”œâ”€â”€ ğŸ“‚ dataset                        # Data implementations
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ image_dataset
â”‚   â”‚   â””â”€â”€ ğŸ“‚ video_dataset
â”‚   â”œâ”€â”€ ğŸ“‚ eval
â”‚   â”œâ”€â”€ ğŸ“‚ model                          # Model implementations for LLaVA
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ language_model
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ image_encoder
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ video_encoder
â”‚   â”‚   â””â”€â”€ ğŸ“‚ multimodal_projector
â”‚   â”œâ”€â”€ ğŸ“‚ serve
â”‚   â”œâ”€â”€ ğŸ“‚ train                          # Train implementations
â”‚   â””â”€â”€ ğŸ“‚ utils
â”œâ”€â”€ ğŸ“‚ playground                         # Train data for video and LLaVA
â”‚   â””â”€â”€ ğŸ“‚ data
â”‚       â”œâ”€â”€ ğŸ“‚ LLaVA-Pretrain
â”‚       â””â”€â”€ ğŸ“‚ video
â””â”€â”€ ğŸ“‚ scripts
```

## ğŸ› ï¸ Installation  

1. Clone this repository and navigate to LLaVA folder
```sh
git clone https://github.com/SeHwanJoo/LLaVA.git
cd LLaVA
```

2. Install Pacakge
```sh
conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip
pip install .
```

3. Install additional packages for training cases
```sh
pip install -e ".[train]"
pip install flash-attn==2.5.5 --no-build-isolation
```

## ğŸ“Š Dataset Preparation  
Download and preprocess datasets before training

If you prefer to `skip downloading` the dataset and only proceed with training, you can skip Dataset Preparation. I've already included a few sample images and videos there as a dummy dataset. 

### ğŸŒ„ Image Dataset (LLaVA)
1. Download `image.zip` from [here](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/images.zip)
2. unzip image.zip following the folder structure below.
```sh
playground/data/
â””â”€â”€ LLaVA-Pretrain
    â”œâ”€â”€ images
    â””â”€â”€ images.zip
```
3. Download `meta_data` from [here](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/blip_laion_cc_sbu_558k.json)
4. Final folder structure should look like this.
```sh
playground/data/
â””â”€â”€ LLaVA-Pretrain
    â”œâ”€â”€ blip_laion_cc_sbu_558k.json
    â””â”€â”€ images
```
Reference: https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md#pretraining-dataset
<details>
<summary>detailed explanation</summary>

The pretraining dataset used in this release is a subset of CC-3M dataset, filtered with a more balanced concept coverage distribution.  Please see [here](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K) for a detailed description of the dataset structure and how to download the images.

If you already have CC-3M dataset on your disk, the image names follow this format: `GCC_train_000000000.jpg`.  You may edit the `image` field correspondingly if necessary.

| Data | Chat File | Meta Data | Size |
| --- |  --- |  --- | ---: |
| CC-3M Concept-balanced 595K | [chat.json](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/chat.json) | [metadata.json](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/metadata.json) | 211 MB
| LAION/CC/SBU BLIP-Caption Concept-balanced 558K | [blip_laion_cc_sbu_558k.json](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/blip_laion_cc_sbu_558k.json) | [metadata.json](#) | 181 MB

**Important notice**: Upon the request from the community, as ~15% images of the original CC-3M dataset are no longer accessible, we upload [`images.zip`](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/images.zip) for better reproducing our work in research community. It must not be used for any other purposes. The use of these images must comply with the CC-3M license. This may be taken down at any time when requested by the original CC-3M dataset owner or owners of the referenced images.
</details>

### ğŸ¥ Video Dataset
1. Download the dataset from [official website](https://www.qualcomm.com/developer/artificial-intelligence/datasets).
2. Preprocess the dataset by changing the video extensoin from `.webm` to `.mp4`. You can use this [script](https://github.com/SeHwanJoo/LLaVA/tree/main/scripts/dataset/webm2mp4.py).
3. Download `train.csv`, `val.csv`, `test.csv` from [here](https://drive.google.com/drive/folders/1cfA-SrPhDB9B8ZckPvnh8D5ysCjD-S_I).
4. Download `something-something-v2-id2label.json` from [here](https://huggingface.co/datasets/huggingface/label-files/blob/main/something-something-v2-id2label.json).
```sh
playground/data/
â””â”€â”€ video
    â”œâ”€â”€ 20bn-something-something-v2-mp4
    â”œâ”€â”€ something-something-v2-id2label.json
    â”œâ”€â”€ test.csv
    â”œâ”€â”€ train.csv
    â””â”€â”€ val.csv
```
5. Preprocess the dataset for instruction format by this [script](https://github.com/SeHwanJoo/LLaVA/tree/main/scripts/dataset/convert2instruct.py). Then final folder structure should look like this.
```sh
playground/data/
â””â”€â”€ video
    â”œâ”€â”€ 20bn-something-something-v2-mp4
    â”œâ”€â”€ something-something-v2-id2label.json
    â”œâ”€â”€ something-something-v2_test.json
    â”œâ”€â”€ something-something-v2_train.json
    â”œâ”€â”€ something-something-v2_val.json
    â”œâ”€â”€ test.csv
    â”œâ”€â”€ train.csv
    â””â”€â”€ val.csv
```


Reference: https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md#pretraining-dataset

## ğŸ‹ï¸â€â™‚ï¸ Training  
To train the model, use the following command:  
```sh
deepspeed --no_local_rank llava/train/train.py --config-name=experiments/pretrain
```
There are various ways to train the model. The following are methods that utilize the predefined configs in the `configs/experiments` directory:
<details> 
<summary>detailed explanation</summary>

1. Training with images
```sh
deepspeed --no_local_rank llava/train/train.py --config-name=experiments/pretrain
```
2. Training with videos
```sh
deepspeed --no_local_rank llava/train/train.py --config-name=experiments/pretrain_video
```
```
3. Training the Phi base model with video
```sh
deepspeed --no_local_rank llava/train/train.py --config-name=experiments/train_phi
```
4. Training the Qwen base model with video
```sh
deepspeed --no_local_rank llava/train/train.py --config-name=experiments/train_qwen
```
</details>

## ğŸ§ª Evaluation  
TBD

## ğŸš€ Model Inference  
TBD

## ğŸ§‘ğŸ»â€ğŸ”§ Model Serve
TBD
