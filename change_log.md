# ðŸ“ Change Log

## ðŸ”„ Changes
- **Refactored Experiment Management**  
  - **Key Points**: `Collaboration`, `Configuration`
  - **Before**: Experiment management was based on script (bash), making it difficult to manage multiple experiments.
  ```bash
  scripts/v1_5
    â”œâ”€â”€ finetune_lora.sh
    â”œâ”€â”€ finetune.sh
    â”œâ”€â”€ finetune_task_lora.sh
    â”œâ”€â”€ finetune_task.sh
    â””â”€â”€ pretrain.sh
  ```
  ```bash
    deepspeed llava/train/train_mem.py \
        --deepspeed ./scripts/zero2.json \
        --model_name_or_path lmsys/vicuna-13b-v1.5 \
        --version plain \
        --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json \
        --image_folder ./playground/data/LLaVA-Pretrain/images \
        --vision_tower openai/clip-vit-large-patch14-336 \
        --mm_projector_type mlp2x_gelu \
        --tune_mm_mlp_adapter True \
        --mm_vision_select_layer -2 \
        --mm_use_im_start_end False \
        --mm_use_im_patch_token False \
        --bf16 True \
        --output_dir ./checkpoints/llava-v1.5-13b-pretrain \
        --num_train_epochs 1 \
        --per_device_train_batch_size 32 \
        --per_device_eval_batch_size 4 \
        --gradient_accumulation_steps 1 \
        --evaluation_strategy "no" \
        --save_strategy "steps" \
        --save_steps 24000 \
        --save_total_limit 1 \
        --learning_rate 1e-3 \
        --weight_decay 0. \
        --warmup_ratio 0.03 \
        --lr_scheduler_type "cosine" \
        --logging_steps 1 \
        --tf32 True \
        --model_max_length 2048 \
        --gradient_checkpointing True \
        --dataloader_num_workers 4 \
        --lazy_preprocess True \
        --report_to wandb
  ```
  - **After**: Replaced with **Hydra-based** configuration for easier multi-experiment setup.
  ```
    configs/
    â”œâ”€â”€ data_args
    â”‚   â”œâ”€â”€ llava_image.yaml
    â”‚   â””â”€â”€ ssv2_video.yaml
    â”œâ”€â”€ experiments
    â”‚   â”œâ”€â”€ finetune_lora.yaml
    â”‚   â”œâ”€â”€ finetune_task_lora.yaml
    â”‚   â”œâ”€â”€ finetune_task.yaml
    â”‚   â”œâ”€â”€ finetune.yaml
    â”‚   â”œâ”€â”€ pretrain.yaml
    â”‚   â””â”€â”€ train_video.yaml
    â”œâ”€â”€ model_args
    â”‚   â”œâ”€â”€ image_encoder
    â”‚   â”‚   â”œâ”€â”€ base.yaml
    â”‚   â”‚   â””â”€â”€ from_adapter.yaml
    â”‚   â”œâ”€â”€ liuhaotian_vicuna-13b.yaml
    â”‚   â””â”€â”€ lmsys_vicuna-13b.yaml
    â””â”€â”€ training_args
        â”œâ”€â”€ finetune_lora.yaml
        â”œâ”€â”€ finetune.yaml
        â””â”€â”€ pretrain.yaml
  ```
  ```yaml
  defaults:
    - /data_args/ssv2_video.yaml
    - /model_args/lmsys_vicuna-13b
    - /training_args/pretrain
    - _self_

    seed: 1997
    training_args:
    output_dir: ./checkpoints/llava-v1.5-13b-pretrain-video
    per_device_train_batch_size: 16
  ```
  - **Expected effect**: 
    - **Enhanced Collaboration**: Modular and well-structured configurations eliminate script-baed complexity, making it easier for teams to collabotate, modify, and scale experiments efficiently.
    - **Improved Experiment Management**: Hydra's hierarachical and reusable configs enable seamless multi-experiment setup, reducing errors and improving ocnsistency across training workflows.

- **Code Cleanup & Structure Refinement** 
  - **Key Points**: `Collaboration` 
  - **Before**: Common functions were scattered across different files, making reuse difficult.
  ```
  llava
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ ðŸ“‚ eval
    â”œâ”€â”€ ðŸ“‚ model
    â”œâ”€â”€ ðŸ“‚ serve
    â”œâ”€â”€ ðŸ“‚ train
    â”‚   â”œâ”€â”€ llama_flash_attn_monkey_patch.py
    â”‚   â”œâ”€â”€ llama_xformers_attn_monkey_patch.py
    â”‚   â”œâ”€â”€ llava_trainer.py
    â”‚   â”œâ”€â”€ train_mem.py
    â”‚   â”œâ”€â”€ train.py
    â”‚   â””â”€â”€ train_xformers.py
    â”œâ”€â”€ constants.py
    â”œâ”€â”€ conversation.py
    â”œâ”€â”€ mm_utils.py
    â””â”€â”€ utils.py
  ```
  - **After**: Organized common utility functions and slightly modified file structure for better maintainability.
  ```
  llava
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ ðŸ“‚ dataset
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ ðŸ“‚ image_dataset
    â”‚   â”œâ”€â”€ ðŸ“‚ video_dataset
    â”‚   â”œâ”€â”€ utils.py
    â”‚   â””â”€â”€ builder.py
    â”œâ”€â”€ ðŸ“‚ eval
    â”œâ”€â”€ ðŸ“‚ model
    â”œâ”€â”€ ðŸ“‚ serve
    â”œâ”€â”€ ðŸ“‚ train
    â”‚   â”œâ”€â”€ llama_flash_attn_monkey_patch.py
    â”‚   â”œâ”€â”€ llava_trainer.py
    â”‚   â”œâ”€â”€ train.py
    â”‚   â””â”€â”€ utils.py
    â””â”€â”€ ðŸ“‚ utils
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ config.py
        â”œâ”€â”€ constants.py
        â”œâ”€â”€ conversation.py
        â”œâ”€â”€ mm_utils.py
        â””â”€â”€ utils.py
  ```
  - **Expected effect**: Enhanced code maintainability and reusability, improving collaboration efficiency.

- **Video Dataset and extendable dataset**
  - **Key Points**: `Data scale-up`, `Collaboration`
  - **Before:** No support for video datasets and multi-datasets configurations.
  - **After**: Added support for **video datasets** and modificate dataset for **scalability**.

  **note: multi-type dataset is not supported**
  
  eg) video dataset and image dataset

  Managed by configs
  ```yaml
    - /data_args:
      - ssv2_video.yaml
      - ssv2_video_2.yaml
  ```
  ```sh
  configs/data_args/
  â”œâ”€â”€ llava_image.yaml
  â””â”€â”€ ssv2_video.yaml
  ```
  Folder structure
  ```sh
  llava/dataset/
  â”œâ”€â”€ __init__.py
  â”œâ”€â”€ builder.py
  â”œâ”€â”€ utils.py
  â”œâ”€â”€ image_dataset
  â”‚   â””â”€â”€ dataset.py
  â””â”€â”€ video_dataset
      â”œâ”€â”€ dataset.py
      â”œâ”€â”€ functional.py
      â”œâ”€â”€ masking_generator.py
      â”œâ”€â”€ rand_augment.py
      â”œâ”€â”€ random_erasing.py
      â””â”€â”€ video_transforms.py
  ```
  - **Expected effect**: 
    - **scalable dataset**: Previously, only one dataset could be used, but now multiple datasets can be trained simultaneously through easy configuration management.
    - **video dataset**: Added support for previously unsupported video datasets.

- **Video Encoder and Extendable Encoder**
  - **Key Points**: `Model Scale-up`, `Collaboration`
  - **Before**: No support for video encoder and multi-encoder.
  - **After**: Added support for **video encoder** and modificate encoder for **scalability**.

    Managed by configuration
    ```yaml
    defaults:
      - image_encoder: base
      - video_encoder: base
      - _self_
    ```
    ```sh
    configs/model_args/
    â”œâ”€â”€ image_encoder
    â”‚   â”œâ”€â”€ base.yaml
    â”‚   â””â”€â”€ from_adapter.yaml
    â””â”€â”€ video_encoder
        â”œâ”€â”€ base.yaml
        â””â”€â”€ from_adapter.yaml
    ```
    Folder structure
    ```sh
    llava/model/
    â”œâ”€â”€ image_encoder
    â”‚   â”œâ”€â”€ builder.py
    â”‚   â””â”€â”€ clip_encoder.py
    â””â”€â”€ video_encoder
        â”œâ”€â”€ builder.py
        â””â”€â”€ clip_encoder.py
    ```
  - **Expected effect**: 
    - **scalable encoder**: Video data includes various features like images, frames, and audio. To effectively process these, multiple encoders are required, and the strucutre has been designed to scale accordingly.
    - **video encoder**: Added support for previously unsupported video encoder.

- **Various LLM**  
  - **Key Points**: `Model Scale-up`
  - **Before**: Only the Llama model was supported, and adding new models required extensive code modifications, as shown below:
  ```python
  class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):
    config_class = LlavaConfig

    def __init__(self, config):
        super(LlamaForCausalLM, self).__init__(config)
        self.model = LlavaLlamaModel(config)
        self.pretraining_tp = config.pretraining_tp
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_model(self):
        return self.model

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        images: Optional[torch.FloatTensor] = None,
        image_sizes: Optional[List[List[int]]] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
      ...skip...

    @torch.no_grad()
    def generate(
        self,
        inputs: Optional[torch.Tensor] = None,
        images: Optional[torch.Tensor] = None,
        image_sizes: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> Union[GenerateOutput, torch.LongTensor]:
      ...skip...

    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,
                                      inputs_embeds=None, **kwargs):
      ...skip...
  ```
  - **After**: Added easy support for adding new LLM models with configuration through a streamlined approach:
  code
  ```python
  class LlavaQwen2ForCausalLM(Qwen2ForCausalLM, LlavaMetaForCausalLM):
    config_class = LlavaQwen2Config

    def __init__(self, config):
        super(Qwen2ForCausalLM, self).__init__(config)
        self.model = LlavaLlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        setup_model_functions(self)

        # Initialize weights and apply final processing
        self.post_init()

    def get_model(self):
        return self.model
  ```
  config
  ```yaml
  class_name: LlavaQwen2ForCausalLM
  model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
  ```
  - **Expected effect**: With the continuous emergence of new LLMs, this approach allows for effortless intergration and experimentation with newly released models.
---

# Future works
- **Efficient Adapter**: AS-IS, only LoRA is supported, but various adapter methods such as DoRA and MoRA are actively being researched. The framework should be extended to suport multiple adapter types while maintain flexible configuration.
- **Feature Project**: AS-IS, extracted image and video features are processed independently. However, to enable joint learning across modalities - including image, video, audio, etc - the framework should be modified to allow seamless intergration of these features.
- **Time Embedding**: Video preprocessing extracts frames based on fixed segments sizes. But, since video durations vary, segmenting by time intervals could be more effective. Incoporating time embeddings could provide benefits similar to positional embeddins enabling better temporal understanding. This approach would also allow handling long videos by segmenting and embedding them.
- **Code Refinement & CleanUp**: The project lacks a standardized code convention, requiring significant refactoring. Unnecessary files should be removed.
- **evaluation & serving**: While modifications have been made to the train pipeline, the evaluation and serving components have not yet been updated accoordingly.
