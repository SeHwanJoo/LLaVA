deepspeed: ./scripts/zero2.json
bf16: True
output_dir: ./checkpoints/llava-v1.5-13b-pretrain
num_train_epochs: 1
per_device_train_batch_size: 32
per_device_eval_batch_size: 4
gradient_accumulation_steps: 1
evaluation_strategy: "no"
save_strategy: "steps"
save_steps: 24000
save_total_limit: 1
learning_rate: 1e-3
weight_decay: 0.
warmup_ratio: 0.03
lr_scheduler_type: "cosine"
logging_steps: 1
tf32: True
model_max_length: 2048
gradient_checkpointing: True
dataloader_num_workers: 4
report_to: wandb
lora_enable: False
lora_r: null
lora_alpha: null
mm_projector_lr: null
attn_implementation: flash_attention_2
local_rank: -1
